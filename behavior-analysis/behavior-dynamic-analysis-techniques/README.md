# Static analysis techniques: the common procedures
In this section we'll start learning about dynamic analysis techniques when looking into malware samples. We will be focusing on the behavior of malware and why we need to dynamically analyze it, the different dynamic analysis methods, the pros and cons of each one, and tools we can use to make this task easier. We'll look at different kinds of malware and their behaviors (Downloaders, droppers, keyloggers...) and go over the methids they use to operate their peyloads, such as injection techniques or LoLBins

But first, let's discuss what exactly is dynamic analysis, the different types of dynamic analysis, when to use each of them and their differences. We'll then dive into the different types of resources used in this task.

## What is dynamic analysis?
Until this point in the notes, we understood we could analyze malware using static methods, wether looking at their signatures, what functions or system APIs they used, or disassembling them and looking at low-level code. But if those do not reveal enough information to answer our questions, we need to go deeper.

For example, if the sample is applying some obfuscation, getting packed with a custom packing mechanism we don't have access to, or extending its capabilities by downloading modules from outside, we'll need to run the sample to gather data about what it actually does at runtime. This is where dynamic analysis comes into play.

Dynamic malware analysis is analyzing malware samples by running them in a contained environment. We will run the malware and see what it does, how it behaves, what exactly it is harming, what operations are being executed and how those are being performed. We will observe the services and processes that are being affected during execution, if the malware is hiding and how.

Another reason it's called dynamic analysis is that there are so many possibilities that the execution can bring, that it is up to the analyst to track, monitor and understand what is happening as the sample runs. As long as we are prepared and understand what we're doing, we'll be able to find some answers.

### Where to run the samples
We can run the samples in several different kinds of isolated environments:
- Virtual Machines, simulated computers inside of a host computer that can run a pre-configured state, saved in a point in time, and can be rolled back as necessary. This ease of use and predictability makes it a preferred option today
- Separate physical systems allow us better simulation of real environments to avoid anti-analysis VM detection, but is a lot more expensive to reproduce. In this case we usually use tools to be able to restore the machine to an earlier state when rebooted, like Shadow Defender, DeepFreeze, Sandboxie or Rollback Rx Professional
- Automated sandbox software allows us to run the samples inside of a snadbox in case we can't have a dedicated environment, which also has the perk of running automated reports on the potential findings it gets from the execution. An example here is Cuckoo, which we will cover later.

### Feeding the malware
An important part of dynamic analysis is adapting to the changes required by the malware. To get the proper information out of it, we may need to change our environment so the malware thinks it has a potential target and runs to its full potential. Make it think it's on its target and not in an isolated environment for analysis.

We can define the states of the contained environment in two categories:
- Insecure environment, meaning we lack many security measures on purpose, to see how the malware acts against an easy target. This also means running the sample as privileged to see its capabilities when running as a high integrity process.
- Hardened environment, where we configure a secure machine we would see in a properly configured environment and see if our real environment would survive an attack from the sample you have. By doing so you may find hidden exploitable misconfigurations or vulnerabilities we could patch.

### When to use each technique
Before we start executing malware, we should have a good understanding of the pros and cons of both static and dynamic malware analysis. Let's cover a couple of points we should keep in mind, but remember it is a good idea to be able to do both, as required to have more tools under your arsenal.

Here's a reference table for these techniques:
```
          --------------------------------------------------------------------------------------------------------
          |              Method          |   Difficulty |                       Notes                            | 
----------|------------------------------|--------------|--------------------------------------------------------|
|         | Header analysis and hashing  | Easy         | Fast / inexpensive                                     |
|         | PE Analysis                  | Intermediate | Fast / inexpensive                                     |
| Static  | Using scanners               | Easy         | Fast / inexpensive                                     |
|         | Reverse Engineering          | Difficult    | Slow / expensive / understanding of assembly           |
|         | Static code analysis         | Difficult    | Slow / expensive / understanding of assembly           |
|---------|---------------------------------------------|--------------------------------------------------------|
|         | Running the malware sample   | Intermediate | Relatively fast / Not very expensive                   |
| Dynamic | Running the sample sandboxed | Easy         | Relatively fast / Not very expensive                   |
|         | Debugging                    | Difficult    | Slow / expensive / Understanding of assembly           |
------------------------------------------------------------------------------------------------------------------
```

### Dynamic analysis methodology
It is always good to have a methodology that you follow and apply. It is not a roadmap that cannot be changed and adapt new paths if needed, but it is a useful starting point for most malware samples, which we will run in the contained environment.

A recommended methodology that is usually used consists of four phases, and includes the four phases:
1. Baseline, where we create the environment with the OS needed, install the tools and take a snapshot we can come back to
2. Pre-Execution, where we perform any necessary configuration, transfer the malware sample to the VM, and start the required tools (monitoring, tracking, and debugging)
3. Post-execution, when we execute the malware, start tracking its behavior and activity (system calls, file access, network traffic), dump/capture screenshots, memory dumps, config files, registry files, unpacked executables...
4. Analysis and documentation, when we analyze and take notes of everything that happened, observe the exhibited behavior, and document events and actions to fill our final report

Often, multiple iterations of phases 2-4 are needed to conclude the analysis.

## Windows processes
Before we start anything related to running a malware, let's dive into the windows system internals and understand the main components of a process and the system we will be doing our investigations and analysis on. This is very important to understand how to track and monitor suspicious processes in your contained environment. We need to understand what's normal first, so we can understand what isn't later.

When reading most of the operating system books, you will see a process is defined as a program in exeution, but it's technically not true, and far more than that. A process is a resource mechanism used to represent a running instance of a program. This resource mechanism is also an object by itself, which is used to manage the resources required by a program in a contained or isolated environment. This means each process has its own environment.

Each process has the following main elements:
- Executable image (the program file), the file that includes the code to be executed. Every process has at least one.
- Private Virtual Address space, where everything the program loads is stored, including the image file, the libraries required, the stack, heap, and other resources. This helps running the process safely without interfering with other processes. The size of this depends on system architecture, process architecture, and the LARGEADDRESSAWARE fkag set to yes or no.
- Private Handle Table, which stores the handles to access resources in the system, like files we want to write to, to interact with the kernel so processes can interact with the system properly. Objects can be many things in a system like registry keys, network sockets, or other processes / Threads
- Access Token, the object that stores the default security context of the process, then grabbed by the thread that actually runs the code. Keep in mind a thread could have a different token later. By default this token includes the security context of the user running the process, plus the groups and permissions of the user.
- Thread(s), the true execution units of a process. They are the ones doing the execution. Each process will have at least one thread and normally will start executing code from the main entrypoint of the executable. Some malware can start executing, however, from TLS (Thread Local Storage). More on that later.

This is how these elements are supposed to be used, however, malware developers may try to exploit weaknesses and cross boundaries, running code in non-conventional ways to avoid detection.

### Process creation steps
Another important execution flow to undersand is how the Windows system creates a process. There are a number of operations that happen in order to create a process, and it is very important to know what they are, their flow, and what happens in each step. Let's go over the process:
1. Check if the file is actually a PE, then open the image file
2. Create and initialize kernel process objects
3. Map image and NTDLL
4. Notify the Windows subsystem process (csrss.exe) (a kernel helper) of a new process and thread
5. Create a Process Environment Block and Thread Environment Block
6. Initialize other process parts needed, like the Heap thread pool
7. Load required DLLs
8. Execute the code's main entrypoint

These will be explored in more detail in coming notes.

## Sysinternals tools
the Sysinternals toolset originally released in 1996 and then was acquired and kept updated by Microsoft. It is a set of utilities for managing, diagnosing, troubleshooting and monitoring a Microsoft Windows environment, but they are also very useful for malware analysts and threat hunters. THe list includes:
- Process explorer, an advanced task manager that could be used to learn about active processes, or what DLLs and handles are opened by them.
- WinObj, which displays the different kernel objects the OS provides and could be accessed by a process
- listDLLs, used to display what DLLs are loaded into a process, similar to the capability of Process Explorer
- Sysinternal Handles tool, a command-line interface to list the handles being used by a certain process
- Procmon, an advanced process monitoring tool that can show, in real-time, what a process or thread activity looks like behind the scenes. It shows events and actions that are happening as the process or thread is active. We'll be using ProcMon in most, if not all, of our basic dynamic analysis. Some useful filters that we can get with procmon are sending or getting data with TCP/UDP, or loading DLLs and executables, creating files, registry activity, or process or thread spawning

Some useful features also available on Procmon include:
- Boot logging, which starts capturing data very early in the booting process, and can be useful to see what activities happen even before user logon, like device driver loading, auto starting services or shell initialization. Even on user logoff or system shutdown
- Drop filtered events reduces the number of captured logs by procmon, as this only stores exactly what we need, using less memory.
- History depth, which stops event capture if the system memory runs too low. We can also configure the count of stored entries.
- Backing files, which stores events in a file automatically in case the system memory runs out so we don't lose that information.

## System processes and services
It's very important to be able to identify what are normal environment processes and what aren't. Once we understand what's running and how, it will be much easier to spot anomalies. This is a threat hunting technique that is also useful here in malware analysis. This is earier with Sysinternals Process Explorer, where we can see the system idle process, memory compression, service control manager, the windows registry (listed, but not truly a process), the System process... you can also notice all process IDs are a multiple of 4. ID 4 always belongs to System. Remember to check your system to know what is considered normal behavior to get a baseline.

It is also important to know where the system images are in case we need to investigate them for legitimacy. 64-bit executables are at C:\Windows\System32, while 32-bit executables that run on the 64-bit program (Through Windows on Windows 64-bit, WoW64) are on C:\Windows\SysWoW64. This redundancy exists because 32-bit processes cannot load 64-bit libraries, and vice-versa, they cannot work without this compatibility emulation. The WoW executables load both the 32 and 64 bit NTDLL entries, as they send requests to the 32-bit one and forward it to the 64-bit library.

To make it easier for file exploration depending on architecture, when you open those directories, Windows actually does a logical path redirection to take you to the proper folder in the file explorer you open, so the directories are actually logical links. Same happens with the registry, the application that accesses the registry gets a logical version of its architecture-specific registry displayed to it. This is important in case you open a 32-bit malware sample in a 64-bit system and it seems to be loading libraries from unexpected locations.

There are some important windows internal files to keep track of, mainly NTDLL.dll which contains internal support functions and system-service helpers, mapping incoming Windows API requests to their corresponding kernel services, and Kernel32.dll, which is a user-mode DLL that passes user requests aimed to the kernel to NTDLL.

## Injection techniques

## Persistent methods

## Tools and automation

## Windows APIs